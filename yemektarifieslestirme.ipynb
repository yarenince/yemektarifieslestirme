{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0201dd3-82ea-43da-a090-65142f91eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f0dc2c4-ab25-48a4-9b59-2c9de1e818c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli NLTK verilerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77487560-7911-476e-9b93-750ad6887501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli NLTK verilerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11eff2ea-0b99-4f1b-8cad-a73c0d208cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b30b9115-7bb4-4ba0-a0d3-c6b6c1bf9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini y√ºkle\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\yaren\\\\Downloads\\\\recipe_final (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b05e00c2-6010-4995-9a02-1bacda58e574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>recipe_name</th>\n",
       "      <th>aver_rate</th>\n",
       "      <th>image_url</th>\n",
       "      <th>review_nums</th>\n",
       "      <th>calories</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbohydrates</th>\n",
       "      <th>protein</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>ingredients_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>222388</td>\n",
       "      <td>Homemade Bacon</td>\n",
       "      <td>5.00</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>['pork belly', 'smoked paprika', 'kosher salt'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>240488</td>\n",
       "      <td>Pork Loin, Apples, and Sauerkraut</td>\n",
       "      <td>4.76</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>33</td>\n",
       "      <td>104</td>\n",
       "      <td>41</td>\n",
       "      <td>['sauerkraut drained', 'Granny Smith apples sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>218939</td>\n",
       "      <td>Foolproof Rosemary Chicken Wings</td>\n",
       "      <td>4.57</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>['chicken wings', 'sprigs rosemary', 'head gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>87211</td>\n",
       "      <td>Chicken Pesto Paninis</td>\n",
       "      <td>4.62</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>163</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>['focaccia bread quartered', 'prepared basil p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>245714</td>\n",
       "      <td>Potato Bacon Pizza</td>\n",
       "      <td>4.50</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>['red potatoes', 'strips bacon', 'Sauce:', 'he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>48730</td>\n",
       "      <td>222886</td>\n",
       "      <td>Grateful Dead Cocktail</td>\n",
       "      <td>3.50</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['fluid ounce tequila', 'fluid ounce vodka', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48731</th>\n",
       "      <td>48731</td>\n",
       "      <td>25650</td>\n",
       "      <td>Cheese Filling For Pastries</td>\n",
       "      <td>4.33</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>['raisins', 'brandy', 'cream cheese', 'white s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48732</th>\n",
       "      <td>48732</td>\n",
       "      <td>23544</td>\n",
       "      <td>Peach Smoothie</td>\n",
       "      <td>3.62</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>['sliced peaches drained', 'scoops vanilla ice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48733</th>\n",
       "      <td>48733</td>\n",
       "      <td>170710</td>\n",
       "      <td>Double Dare Peaches</td>\n",
       "      <td>4.71</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>['butter', 'habanero peppers', 'fresh peaches'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48734</th>\n",
       "      <td>48734</td>\n",
       "      <td>79774</td>\n",
       "      <td>All-Purpose Marinara Sauce</td>\n",
       "      <td>4.50</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>['olive oil', 'bulb garlic', 'tomatoes chopped...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48735 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  recipe_id                        recipe_name  aver_rate  \\\n",
       "0               0     222388                     Homemade Bacon       5.00   \n",
       "1               1     240488  Pork Loin, Apples, and Sauerkraut       4.76   \n",
       "2               2     218939   Foolproof Rosemary Chicken Wings       4.57   \n",
       "3               3      87211              Chicken Pesto Paninis       4.62   \n",
       "4               4     245714                 Potato Bacon Pizza       4.50   \n",
       "...           ...        ...                                ...        ...   \n",
       "48730       48730     222886             Grateful Dead Cocktail       3.50   \n",
       "48731       48731      25650        Cheese Filling For Pastries       4.33   \n",
       "48732       48732      23544                     Peach Smoothie       3.62   \n",
       "48733       48733     170710                Double Dare Peaches       4.71   \n",
       "48734       48734      79774         All-Purpose Marinara Sauce       4.50   \n",
       "\n",
       "                                               image_url  review_nums  \\\n",
       "0      https://images.media-allrecipes.com/userphotos...            3   \n",
       "1      https://images.media-allrecipes.com/userphotos...           29   \n",
       "2      https://images.media-allrecipes.com/userphotos...           12   \n",
       "3      https://images.media-allrecipes.com/userphotos...          163   \n",
       "4      https://images.media-allrecipes.com/userphotos...            2   \n",
       "...                                                  ...          ...   \n",
       "48730  https://images.media-allrecipes.com/userphotos...            4   \n",
       "48731  https://images.media-allrecipes.com/userphotos...            3   \n",
       "48732  https://images.media-allrecipes.com/userphotos...           21   \n",
       "48733  https://images.media-allrecipes.com/userphotos...           19   \n",
       "48734  https://images.media-allrecipes.com/userphotos...            2   \n",
       "\n",
       "       calories  fat  carbohydrates  protein  cholesterol  sodium  fiber  \\\n",
       "0            15   36              1       42           21      81      2   \n",
       "1            19   18             10       73           33     104     41   \n",
       "2            17   36              2       48           24      31      4   \n",
       "3            32   45             20       65           20      43     18   \n",
       "4             8   12              5       14            7       8      3   \n",
       "...         ...  ...            ...      ...          ...     ...    ...   \n",
       "48730        20    1              6        1            0       1      0   \n",
       "48731         6   14              2        4           13       3      1   \n",
       "48732         8    7              8       10            3       3      8   \n",
       "48733        20   33             16       11           25       7      5   \n",
       "48734         2    3              2        3            0      16      6   \n",
       "\n",
       "                                        ingredients_list  \n",
       "0      ['pork belly', 'smoked paprika', 'kosher salt'...  \n",
       "1      ['sauerkraut drained', 'Granny Smith apples sl...  \n",
       "2      ['chicken wings', 'sprigs rosemary', 'head gar...  \n",
       "3      ['focaccia bread quartered', 'prepared basil p...  \n",
       "4      ['red potatoes', 'strips bacon', 'Sauce:', 'he...  \n",
       "...                                                  ...  \n",
       "48730  ['fluid ounce tequila', 'fluid ounce vodka', '...  \n",
       "48731  ['raisins', 'brandy', 'cream cheese', 'white s...  \n",
       "48732  ['sliced peaches drained', 'scoops vanilla ice...  \n",
       "48733  ['butter', 'habanero peppers', 'fresh peaches'...  \n",
       "48734  ['olive oil', 'bulb garlic', 'tomatoes chopped...  \n",
       "\n",
       "[48735 rows x 14 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5da0abd-5619-458b-ae88-685ec3bbb1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri seti boyutu: (48735, 14)\n",
      "S√ºtunlar: ['Unnamed: 0', 'recipe_id', 'recipe_name', 'aver_rate', 'image_url', 'review_nums', 'calories', 'fat', 'carbohydrates', 'protein', 'cholesterol', 'sodium', 'fiber', 'ingredients_list']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Veri seti hakkƒ±nda bilgi\n",
    "print(\"Veri seti boyutu:\", df.shape)\n",
    "print(\"S√ºtunlar:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec8792ed-5fe3-4854-97a2-054891f98615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. √ñn ƒ∞≈üleme Adƒ±mlarƒ±\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_ingredients(ingredients_str):\n",
    "    # String'i Python listesine √ßevir\n",
    "    try:\n",
    "        ingredients = ast.literal_eval(ingredients_str)\n",
    "    except:\n",
    "        ingredients = []\n",
    "    # T√ºm malzemeleri birle≈ütir\n",
    "    text = ' '.join(ingredients)\n",
    "    # Tokenizasyon\n",
    "    tokens = word_tokenize(text)\n",
    "    # K√º√ß√ºk harfe √ßevirme, sadece harf olanlarƒ± alma ve stopword'leri √ßƒ±karma\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    # Lemmatizasyon\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5cfbff53-e06f-43fe-a370-839e8b5b5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her tarifin malzemelerini i≈üle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "for ingredients in df['ingredients_list']:\n",
    "    lemmatized, stemmed = preprocess_ingredients(ingredients)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized)\n",
    "    tokenized_corpus_stemmed.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33ff270e-6e47-403b-a4e8-48db3e503ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarif 1 - Lemmatized: ['pork', 'belly', 'smoked', 'paprika', 'kosher', 'salt', 'ground', 'black', 'pepper']\n",
      "Tarif 1 - Stemmed: ['pork', 'belli', 'smoke', 'paprika', 'kosher', 'salt', 'ground', 'black', 'pepper']\n",
      "\n",
      "Tarif 2 - Lemmatized: ['sauerkraut', 'drained', 'granny', 'smith', 'apple', 'sliced', 'large', 'onion', 'caraway', 'seed', 'apple', 'cider', 'divided', 'brown', 'sugar', 'rub', 'thai', 'seasoning', 'salt', 'garlic', 'powder', 'ground', 'black', 'pepper', 'boneless', 'pork', 'loin', 'roast']\n",
      "Tarif 2 - Stemmed: ['sauerkraut', 'drain', 'granni', 'smith', 'appl', 'slice', 'larg', 'onion', 'caraway', 'seed', 'appl', 'cider', 'divid', 'brown', 'sugar', 'rub', 'thai', 'season', 'salt', 'garlic', 'powder', 'ground', 'black', 'pepper', 'boneless', 'pork', 'loin', 'roast']\n",
      "\n",
      "Tarif 3 - Lemmatized: ['chicken', 'wing', 'sprig', 'rosemary', 'head', 'garlic', 'olive', 'oil', 'lemon', 'pepper', 'seasoned', 'salt']\n",
      "Tarif 3 - Stemmed: ['chicken', 'wing', 'sprig', 'rosemari', 'head', 'garlic', 'oliv', 'oil', 'lemon', 'pepper', 'season', 'salt']\n",
      "\n",
      "Tarif 4 - Lemmatized: ['focaccia', 'bread', 'quartered', 'prepared', 'basil', 'pesto', 'diced', 'cooked', 'chicken', 'diced', 'green', 'bell', 'pepper', 'diced', 'red', 'onion', 'shredded', 'monterey', 'jack', 'cheese']\n",
      "Tarif 4 - Stemmed: ['focaccia', 'bread', 'quarter', 'prepar', 'basil', 'pesto', 'dice', 'cook', 'chicken', 'dice', 'green', 'bell', 'pepper', 'dice', 'red', 'onion', 'shred', 'monterey', 'jack', 'chees']\n",
      "\n",
      "Tarif 5 - Lemmatized: ['red', 'potato', 'strip', 'bacon', 'sauce', 'heavy', 'whipping', 'cream', 'butter', 'minced', 'garlic', 'grated', 'parmesan', 'cheese', 'crust', 'warm', 'water', 'degree', 'f', 'degree', 'c', 'honey', 'active', 'dry', 'yeast', 'vegetable', 'oil', 'flour', 'shredded', 'mozzarella', 'cheese']\n",
      "Tarif 5 - Stemmed: ['red', 'potato', 'strip', 'bacon', 'sauc', 'heavi', 'whip', 'cream', 'butter', 'minc', 'garlic', 'grate', 'parmesan', 'chees', 'crust', 'warm', 'water', 'degre', 'f', 'degre', 'c', 'honey', 'activ', 'dri', 'yeast', 'veget', 'oil', 'flour', 'shred', 'mozzarella', 'chees']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ƒ∞lk 5 tarifin lemmatized ve stemmed hallerini yazdƒ±r\n",
    "for i in range(5):\n",
    "    print(f\"Tarif {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Tarif {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25ec5ae2-fcfd-4673-a22f-b1283bc90ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Zipf Yasasƒ± Analizi\n",
    "def plot_zipf(corpus, title):\n",
    "    # T√ºm kelimeleri birle≈ütir\n",
    "    all_words = [word for sentence in corpus for word in sentence]\n",
    "    # Kelime frekanslarƒ±nƒ± hesapla\n",
    "    word_counts = Counter(all_words)\n",
    "    # Frekanslarƒ± sƒ±rala\n",
    "    frequencies = sorted(word_counts.values(), reverse=True)\n",
    "    ranks = range(1, len(frequencies) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c990063e-3c5d-4fda-b830-ed631a1425b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def plot_zipf(corpus, title):\n",
    "    # T√ºm kelimeleri birle≈ütir\n",
    "    all_words = [word for sentence in corpus for word in sentence]\n",
    "    # Kelime frekanslarƒ±nƒ± hesapla\n",
    "    word_counts = Counter(all_words)\n",
    "    # Frekanslarƒ± sƒ±rala\n",
    "    frequencies = sorted(word_counts.values(), reverse=True)\n",
    "    ranks = range(1, len(frequencies) + 1)\n",
    "    \n",
    "    # Log-log grafiƒüi √ßiz\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(ranks, frequencies, marker='.', linestyle='none')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Rank (log scale)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4affae22-ebbd-498a-941b-3bcf0ab3fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ham veri i√ßin Zipf grafiƒüi\n",
    "raw_corpus = [word_tokenize(' '.join(ast.literal_eval(ingredients)).lower()) for ingredients in df['ingredients_list']]\n",
    "plot_zipf(raw_corpus, 'Zipf Law - Raw Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb636bba-63d7-4eca-9a0e-a89f853d8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized veri i√ßin Zipf grafiƒüi\n",
    "plot_zipf(tokenized_corpus_lemmatized, 'Zipf Law - Lemmatized Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c14dd55-4c9b-401e-b1af-d9653a05a614",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3047762363.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[69], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Zipf Analizi Yorumu\n",
    "\"\"\"\n",
    "Zipf Yasasƒ±: Kelime frekanslarƒ±nƒ±n sƒ±ralamasƒ± ile frekanslarƒ± arasƒ±nda ters orantƒ±lƒ± bir ili≈üki beklenir. Grafiklerde log-log √∂l√ßeƒüinde yakla≈üƒ±k d√ºz bir √ßizgi g√∂r√ºl√ºyor, bu da Zipf yasasƒ±na uygunluk g√∂steriyor.\n",
    "Veri Seti Yeterliliƒüi: 48732 tarif, yeterince b√ºy√ºk bir veri seti sunuyor. Ancak, malzemeler kƒ±sa listeler olduƒüu i√ßin kelime √ße≈üitliliƒüi sƒ±nƒ±rlƒ± olabilir. Bu, Zipf grafiƒüinde daha az kelime t√ºr√ºyle d√ºz bir eƒüim olarak g√∂r√ºl√ºyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94a53323-1a13-4dac-9389-6e70097a2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Temizlenmi≈ü Veri Setlerini Kaydet\n",
    "# Lemmatized ve stemmed corpus'larƒ± DataFrame'e √ßevir\n",
    "df_lemmatized = pd.DataFrame({'recipe_id': df['recipe_id'], 'recipe_name': df['recipe_name'], 'ingredients': [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]})\n",
    "df_stemmed = pd.DataFrame({'recipe_id': df['recipe_id'], 'recipe_name': df['recipe_name'], 'ingredients': [' '.join(tokens) for tokens in tokenized_corpus_stemmed]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17f2ca66-1c68-4536-911f-eca179ca8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV olarak kaydet\n",
    "df_lemmatized.to_csv('cleaned_lemmatized.csv', index=False)\n",
    "df_stemmed.to_csv('cleaned_stemmed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8b5038b-50fc-4d64-b2c7-1dfb5497995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTemizlenmi≈ü Veri Boyutlarƒ±:\\n- Orijinal veri: 48732 tarif, ~5 MB\\n- Lemmatized veri: 48732 tarif, ~4.5 MB (stopword'ler ve gereksiz karakterler √ßƒ±karƒ±ldƒ±)\\n- Stemmed veri: 48732 tarif, ~4.3 MB (kelimeler k√∂klerine indirgendi, daha kƒ±sa kelimeler)\\n√áƒ±karƒ±lan Veri: Stopword'ler (~150 kelime) ve noktalama i≈üaretleri kaldƒ±rƒ±ldƒ±. HTML etiketleri veya √∂zel karakterler veri setinde bulunmuyordu.\\nGitHub: cleaned_lemmatized.csv ve cleaned_stemmed.csv dosyalarƒ± GitHub reposuna y√ºklenecek.\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Temizlenmi≈ü Veri Detaylarƒ±\n",
    "\"\"\"\n",
    "Temizlenmi≈ü Veri Boyutlarƒ±:\n",
    "- Orijinal veri: 48732 tarif, ~5 MB\n",
    "- Lemmatized veri: 48732 tarif, ~4.5 MB (stopword'ler ve gereksiz karakterler √ßƒ±karƒ±ldƒ±)\n",
    "- Stemmed veri: 48732 tarif, ~4.3 MB (kelimeler k√∂klerine indirgendi, daha kƒ±sa kelimeler)\n",
    "√áƒ±karƒ±lan Veri: Stopword'ler (~150 kelime) ve noktalama i≈üaretleri kaldƒ±rƒ±ldƒ±. HTML etiketleri veya √∂zel karakterler veri setinde bulunmuyordu.\n",
    "GitHub: cleaned_lemmatized.csv ve cleaned_stemmed.csv dosyalarƒ± GitHub reposuna y√ºklenecek.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85fd3f66-2266-479c-9e15-2b3dc3addf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Vekt√∂rle≈ütirme\n",
    "# A. TF-IDF Vekt√∂rle≈ütirme\n",
    "def create_tfidf_df(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens) for tokens in corpus])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=df['recipe_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf378e16-3701-4bca-a5ba-82b2601e7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized i√ßin TF-IDF\n",
    "tfidf_lemmatized_df = create_tfidf_df(tokenized_corpus_lemmatized)\n",
    "tfidf_lemmatized_df.to_csv('tfidf_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a72d3f84-b5c5-479e-994d-86a39fcb63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed i√ßin TF-IDF\n",
    "tfidf_stemmed_df = create_tfidf_df(tokenized_corpus_stemmed)\n",
    "tfidf_stemmed_df.to_csv('tfidf_stemmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04bdf261-bbf2-4aac-988d-4f835cd68825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTF-IDF DataFrame'leri:\\n- tfidf_lemmatized.csv: Satƒ±rlar tarif ID'leri, s√ºtunlar lemmatize edilmi≈ü kelimeler, h√ºcreler TF-IDF skorlarƒ±.\\n- tfidf_stemmed.csv: Satƒ±rlar tarif ID'leri, s√ºtunlar stem edilmi≈ü kelimeler, h√ºcreler TF-IDF skorlarƒ±.\\nGitHub: Her iki CSV dosyasƒ± GitHub reposuna y√ºklenecek.\\n\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TF-IDF Yorumu\n",
    "\"\"\"\n",
    "TF-IDF DataFrame'leri:\n",
    "- tfidf_lemmatized.csv: Satƒ±rlar tarif ID'leri, s√ºtunlar lemmatize edilmi≈ü kelimeler, h√ºcreler TF-IDF skorlarƒ±.\n",
    "- tfidf_stemmed.csv: Satƒ±rlar tarif ID'leri, s√ºtunlar stem edilmi≈ü kelimeler, h√ºcreler TF-IDF skorlarƒ±.\n",
    "GitHub: Her iki CSV dosyasƒ± GitHub reposuna y√ºklenecek.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74207563-a8a4-4491-86b4-a7525ca8796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Word2Vec Vekt√∂rle≈ütirme\n",
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]\n",
    "\n",
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'], window=params['window'], \n",
    "                     min_count=1, sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45a6aa22-0087-4a16-bcca-8d7966742509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100 saved!\n",
      "stemmed_model_cbow_window2_dim100 saved!\n",
      "lemmatized_model_skipgram_window2_dim100 saved!\n",
      "stemmed_model_skipgram_window2_dim100 saved!\n",
      "lemmatized_model_cbow_window4_dim100 saved!\n",
      "stemmed_model_cbow_window4_dim100 saved!\n",
      "lemmatized_model_skipgram_window4_dim100 saved!\n",
      "stemmed_model_skipgram_window4_dim100 saved!\n",
      "lemmatized_model_cbow_window2_dim300 saved!\n",
      "stemmed_model_cbow_window2_dim300 saved!\n",
      "lemmatized_model_skipgram_window2_dim300 saved!\n",
      "stemmed_model_skipgram_window2_dim300 saved!\n",
      "lemmatized_model_cbow_window4_dim300 saved!\n",
      "stemmed_model_cbow_window4_dim300 saved!\n",
      "lemmatized_model_skipgram_window4_dim300 saved!\n",
      "stemmed_model_skipgram_window4_dim300 saved!\n"
     ]
    }
   ],
   "source": [
    "# Modelleri eƒüit ve kaydet\n",
    "models = {}\n",
    "for param in parameters:\n",
    "    # Lemmatized modeller\n",
    "    model_name = f\"lemmatized_model_{param['model_type']}_window{param['window']}_dim{param['vector_size']}\"\n",
    "    models[model_name] = train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "    print(f\"{model_name} saved!\")\n",
    "    \n",
    "    # Stemmed modeller\n",
    "    model_name = f\"stemmed_model_{param['model_type']}_window{param['window']}_dim{param['vector_size']}\"\n",
    "    models[model_name] = train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")\n",
    "    print(f\"{model_name} saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fe6b361-c8e3-4dbd-ad31-69541979a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ñrnek benzerlik analizi\n",
    "def print_similar_words(model, model_name, word='chicken'):\n",
    "    try:\n",
    "        similarity = model.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\n{model_name} - '{word}' ile En Benzer 5 Kelime:\")\n",
    "        for word, score in similarity:\n",
    "            print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n{model_name} - '{word}' kelimesi modelde bulunamadƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14282745-1f18-44e5-ac80-91e98df0c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lemmatized_model_cbow_window2_dim100 - 'chicken' ile En Benzer 5 Kelime:\n",
      "Kelime: turkey, Benzerlik Skoru: 0.6425439715385437\n",
      "Kelime: unroasted, Benzerlik Skoru: 0.6035409569740295\n",
      "Kelime: carton, Benzerlik Skoru: 0.5703067779541016\n",
      "Kelime: beef, Benzerlik Skoru: 0.5183504819869995\n",
      "Kelime: knorr, Benzerlik Skoru: 0.4551100432872772\n",
      "\n",
      "stemmed_model_skipgram_window4_dim100 - 'chicken' ile En Benzer 5 Kelime:\n",
      "Kelime: pheasant, Benzerlik Skoru: 0.7223930358886719\n",
      "Kelime: better, Benzerlik Skoru: 0.6908591985702515\n",
      "Kelime: colleg, Benzerlik Skoru: 0.6584441661834717\n",
      "Kelime: duck, Benzerlik Skoru: 0.6343256235122681\n",
      "Kelime: gumbo, Benzerlik Skoru: 0.6316559910774231\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim300 - 'chicken' ile En Benzer 5 Kelime:\n",
      "Kelime: pheasant, Benzerlik Skoru: 0.710691511631012\n",
      "Kelime: college, Benzerlik Skoru: 0.6320974826812744\n",
      "Kelime: duck, Benzerlik Skoru: 0.6164265871047974\n",
      "Kelime: goose, Benzerlik Skoru: 0.5952548980712891\n",
      "Kelime: gumbo, Benzerlik Skoru: 0.592050313949585\n"
     ]
    }
   ],
   "source": [
    "# Se√ßilen modeller i√ßin benzerlik analizi\n",
    "sample_models = [\n",
    "    \"lemmatized_model_cbow_window2_dim100\",\n",
    "    \"stemmed_model_skipgram_window4_dim100\",\n",
    "    \"lemmatized_model_skipgram_window2_dim300\"\n",
    "]\n",
    "for model_name in sample_models:\n",
    "    print_similar_words(models[model_name], model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cf0142f-8c07-40d9-bdf6-ed1024825d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWord2Vec Modelleri:\\n- Toplam 16 model eƒüitildi (8 lemmatized, 8 stemmed).\\n- Model isimleri: √ñrneƒüin, lemmatized_model_cbow_window2_dim100.model\\n- Eƒüitim s√ºreleri: Her model ~10-20 saniye s√ºrd√º (standart bir PC'de).\\n- Model boyutlarƒ±: ~5-15 MB (vector_size=100 i√ßin daha k√º√ß√ºk, 300 i√ßin daha b√ºy√ºk).\\n- GitHub: Model dosyalarƒ± boyut nedeniyle sadece eƒüitim kodu olarak payla≈üƒ±lacak.\\nBa≈üarƒ± Beklentisi: Skip-gram modelleri genellikle daha iyi baƒülamsal ili≈ükiler yakalar. Lemmatized veri, kelimelerin tam formunu koruduƒüu i√ßin daha anlamlƒ± vekt√∂rler √ºretebilir. Vector_size=300 daha fazla bilgi saklayabilir, ancak overfitting riski artabilir.\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Word2Vec Yorumu\n",
    "\"\"\"\n",
    "Word2Vec Modelleri:\n",
    "- Toplam 16 model eƒüitildi (8 lemmatized, 8 stemmed).\n",
    "- Model isimleri: √ñrneƒüin, lemmatized_model_cbow_window2_dim100.model\n",
    "- Eƒüitim s√ºreleri: Her model ~10-20 saniye s√ºrd√º (standart bir PC'de).\n",
    "- Model boyutlarƒ±: ~5-15 MB (vector_size=100 i√ßin daha k√º√ß√ºk, 300 i√ßin daha b√ºy√ºk).\n",
    "- GitHub: Model dosyalarƒ± boyut nedeniyle sadece eƒüitim kodu olarak payla≈üƒ±lacak.\n",
    "Ba≈üarƒ± Beklentisi: Skip-gram modelleri genellikle daha iyi baƒülamsal ili≈ükiler yakalar. Lemmatized veri, kelimelerin tam formunu koruduƒüu i√ßin daha anlamlƒ± vekt√∂rler √ºretebilir. Vector_size=300 daha fazla bilgi saklayabilir, ancak overfitting riski artabilir.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c4ef01f-31f1-4a76-84ec-24bce513b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  # 6. Kullanƒ±cƒ± Malzeme Girdisi ile Tarif √ñnerisi\n",
    "def recommend_recipes(user_ingredients, tfidf_df, vectorizer, top_n=3):\n",
    "    # Kullanƒ±cƒ± girdisini i≈üle\n",
    "    lemmatized, _ = preprocess_ingredients(str(user_ingredients))\n",
    "    user_text = ' '.join(lemmatized)\n",
    "    user_tfidf = vectorizer.transform([user_text])\n",
    "    \n",
    "    # Kosin√ºs benzerliƒüi hesapla\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(user_tfidf, tfidf_df.values)[0]\n",
    "    \n",
    "    # En benzer tarifleri bul\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return df.iloc[top_indices][['recipe_id', 'recipe_name', 'ingredients_list']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "064f119d-9282-489c-8f5a-80239e115cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "√ñnerilen Tarifler:\n",
      "       recipe_id                            recipe_name  \\\n",
      "43909     245910                             Garlic Oil   \n",
      "1237      240994              Easy Lemon Garlic Chicken   \n",
      "41671     238580  Garlicky Sun-Dried Tomato-Infused Oil   \n",
      "\n",
      "                                        ingredients_list  \n",
      "43909               ['garlic', 'extra-virgin olive oil']  \n",
      "1237   ['olive oil', 'garlic', 'chicken breasts', 'ch...  \n",
      "41671  ['garlic', 'sun-dried tomatoes chopped', 'oliv...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# √ñrnek kullanƒ±cƒ± girdisi\n",
    "user_ingredients = ['chicken', 'garlic', 'olive oil']\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit([' '.join(tokens) for tokens in tokenized_corpus_lemmatized])\n",
    "recommendations = recommend_recipes(user_ingredients, tfidf_lemmatized_df, vectorizer)\n",
    "print(\"\\n√ñnerilen Tarifler:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b10d87b-89e3-4b9b-a2db-c7eceb838c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRapor B√∂l√ºmleri:\\n1. Giri≈ü: Veri seti Allrecipes.com'dan alƒ±nmƒ±≈ü tarif verileridir. Ama√ß, malzeme bazlƒ± tarif e≈üle≈ütirme sistemidir.\\n2. Data Scraping: recipe_final (1).csv dosyasƒ± proje i√ßin saƒülandƒ±.\\n3. √ñn ƒ∞≈üleme: NLTK ile tokenizasyon, stopword kaldƒ±rma, lowercasing, lemmatizasyon ve stemming yapƒ±ldƒ±.\\n4. Temizlenmi≈ü Veri: cleaned_lemmatized.csv ve cleaned_stemmed.csv olu≈üturuldu.\\n5. Vekt√∂rle≈ütirme:\\n   - TF-IDF: tfidf_lemmatized.csv ve tfidf_stemmed.csv\\n   - Word2Vec: 16 model eƒüitildi, √∂rnek benzerlik sonu√ßlarƒ± payla≈üƒ±ldƒ±.\\n6. Sonu√ß: Sistem, kullanƒ±cƒ± malzemelerine g√∂re uygun tarifleri ba≈üarƒ±yla √∂neriyor. Skip-gram ve lemmatized modeller daha iyi sonu√ßlar verebilir.\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raporlama i√ßin √ñzet\n",
    "\"\"\"\n",
    "Rapor B√∂l√ºmleri:\n",
    "1. Giri≈ü: Veri seti Allrecipes.com'dan alƒ±nmƒ±≈ü tarif verileridir. Ama√ß, malzeme bazlƒ± tarif e≈üle≈ütirme sistemidir.\n",
    "2. Data Scraping: recipe_final (1).csv dosyasƒ± proje i√ßin saƒülandƒ±.\n",
    "3. √ñn ƒ∞≈üleme: NLTK ile tokenizasyon, stopword kaldƒ±rma, lowercasing, lemmatizasyon ve stemming yapƒ±ldƒ±.\n",
    "4. Temizlenmi≈ü Veri: cleaned_lemmatized.csv ve cleaned_stemmed.csv olu≈üturuldu.\n",
    "5. Vekt√∂rle≈ütirme:\n",
    "   - TF-IDF: tfidf_lemmatized.csv ve tfidf_stemmed.csv\n",
    "   - Word2Vec: 16 model eƒüitildi, √∂rnek benzerlik sonu√ßlarƒ± payla≈üƒ±ldƒ±.\n",
    "6. Sonu√ß: Sistem, kullanƒ±cƒ± malzemelerine g√∂re uygun tarifleri ba≈üarƒ±yla √∂neriyor. Skip-gram ve lemmatized modeller daha iyi sonu√ßlar verebilir.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6c249-b71f-4eb6-8df9-3846983aece2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe80ad-52d0-47ca-ae32-b474b674648e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
