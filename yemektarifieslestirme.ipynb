{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0201dd3-82ea-43da-a090-65142f91eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f0dc2c4-ab25-48a4-9b59-2c9de1e818c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli NLTK verilerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77487560-7911-476e-9b93-750ad6887501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gerekli NLTK verilerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11eff2ea-0b99-4f1b-8cad-a73c0d208cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\yaren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b30b9115-7bb4-4ba0-a0d3-c6b6c1bf9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini yükle\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\yaren\\\\Downloads\\\\recipe_final (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b05e00c2-6010-4995-9a02-1bacda58e574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>recipe_name</th>\n",
       "      <th>aver_rate</th>\n",
       "      <th>image_url</th>\n",
       "      <th>review_nums</th>\n",
       "      <th>calories</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbohydrates</th>\n",
       "      <th>protein</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>ingredients_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>222388</td>\n",
       "      <td>Homemade Bacon</td>\n",
       "      <td>5.00</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>['pork belly', 'smoked paprika', 'kosher salt'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>240488</td>\n",
       "      <td>Pork Loin, Apples, and Sauerkraut</td>\n",
       "      <td>4.76</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>33</td>\n",
       "      <td>104</td>\n",
       "      <td>41</td>\n",
       "      <td>['sauerkraut drained', 'Granny Smith apples sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>218939</td>\n",
       "      <td>Foolproof Rosemary Chicken Wings</td>\n",
       "      <td>4.57</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>['chicken wings', 'sprigs rosemary', 'head gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>87211</td>\n",
       "      <td>Chicken Pesto Paninis</td>\n",
       "      <td>4.62</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>163</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>['focaccia bread quartered', 'prepared basil p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>245714</td>\n",
       "      <td>Potato Bacon Pizza</td>\n",
       "      <td>4.50</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>['red potatoes', 'strips bacon', 'Sauce:', 'he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48730</th>\n",
       "      <td>48730</td>\n",
       "      <td>222886</td>\n",
       "      <td>Grateful Dead Cocktail</td>\n",
       "      <td>3.50</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['fluid ounce tequila', 'fluid ounce vodka', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48731</th>\n",
       "      <td>48731</td>\n",
       "      <td>25650</td>\n",
       "      <td>Cheese Filling For Pastries</td>\n",
       "      <td>4.33</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>['raisins', 'brandy', 'cream cheese', 'white s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48732</th>\n",
       "      <td>48732</td>\n",
       "      <td>23544</td>\n",
       "      <td>Peach Smoothie</td>\n",
       "      <td>3.62</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>['sliced peaches drained', 'scoops vanilla ice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48733</th>\n",
       "      <td>48733</td>\n",
       "      <td>170710</td>\n",
       "      <td>Double Dare Peaches</td>\n",
       "      <td>4.71</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>['butter', 'habanero peppers', 'fresh peaches'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48734</th>\n",
       "      <td>48734</td>\n",
       "      <td>79774</td>\n",
       "      <td>All-Purpose Marinara Sauce</td>\n",
       "      <td>4.50</td>\n",
       "      <td>https://images.media-allrecipes.com/userphotos...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>['olive oil', 'bulb garlic', 'tomatoes chopped...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48735 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  recipe_id                        recipe_name  aver_rate  \\\n",
       "0               0     222388                     Homemade Bacon       5.00   \n",
       "1               1     240488  Pork Loin, Apples, and Sauerkraut       4.76   \n",
       "2               2     218939   Foolproof Rosemary Chicken Wings       4.57   \n",
       "3               3      87211              Chicken Pesto Paninis       4.62   \n",
       "4               4     245714                 Potato Bacon Pizza       4.50   \n",
       "...           ...        ...                                ...        ...   \n",
       "48730       48730     222886             Grateful Dead Cocktail       3.50   \n",
       "48731       48731      25650        Cheese Filling For Pastries       4.33   \n",
       "48732       48732      23544                     Peach Smoothie       3.62   \n",
       "48733       48733     170710                Double Dare Peaches       4.71   \n",
       "48734       48734      79774         All-Purpose Marinara Sauce       4.50   \n",
       "\n",
       "                                               image_url  review_nums  \\\n",
       "0      https://images.media-allrecipes.com/userphotos...            3   \n",
       "1      https://images.media-allrecipes.com/userphotos...           29   \n",
       "2      https://images.media-allrecipes.com/userphotos...           12   \n",
       "3      https://images.media-allrecipes.com/userphotos...          163   \n",
       "4      https://images.media-allrecipes.com/userphotos...            2   \n",
       "...                                                  ...          ...   \n",
       "48730  https://images.media-allrecipes.com/userphotos...            4   \n",
       "48731  https://images.media-allrecipes.com/userphotos...            3   \n",
       "48732  https://images.media-allrecipes.com/userphotos...           21   \n",
       "48733  https://images.media-allrecipes.com/userphotos...           19   \n",
       "48734  https://images.media-allrecipes.com/userphotos...            2   \n",
       "\n",
       "       calories  fat  carbohydrates  protein  cholesterol  sodium  fiber  \\\n",
       "0            15   36              1       42           21      81      2   \n",
       "1            19   18             10       73           33     104     41   \n",
       "2            17   36              2       48           24      31      4   \n",
       "3            32   45             20       65           20      43     18   \n",
       "4             8   12              5       14            7       8      3   \n",
       "...         ...  ...            ...      ...          ...     ...    ...   \n",
       "48730        20    1              6        1            0       1      0   \n",
       "48731         6   14              2        4           13       3      1   \n",
       "48732         8    7              8       10            3       3      8   \n",
       "48733        20   33             16       11           25       7      5   \n",
       "48734         2    3              2        3            0      16      6   \n",
       "\n",
       "                                        ingredients_list  \n",
       "0      ['pork belly', 'smoked paprika', 'kosher salt'...  \n",
       "1      ['sauerkraut drained', 'Granny Smith apples sl...  \n",
       "2      ['chicken wings', 'sprigs rosemary', 'head gar...  \n",
       "3      ['focaccia bread quartered', 'prepared basil p...  \n",
       "4      ['red potatoes', 'strips bacon', 'Sauce:', 'he...  \n",
       "...                                                  ...  \n",
       "48730  ['fluid ounce tequila', 'fluid ounce vodka', '...  \n",
       "48731  ['raisins', 'brandy', 'cream cheese', 'white s...  \n",
       "48732  ['sliced peaches drained', 'scoops vanilla ice...  \n",
       "48733  ['butter', 'habanero peppers', 'fresh peaches'...  \n",
       "48734  ['olive oil', 'bulb garlic', 'tomatoes chopped...  \n",
       "\n",
       "[48735 rows x 14 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5da0abd-5619-458b-ae88-685ec3bbb1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri seti boyutu: (48735, 14)\n",
      "Sütunlar: ['Unnamed: 0', 'recipe_id', 'recipe_name', 'aver_rate', 'image_url', 'review_nums', 'calories', 'fat', 'carbohydrates', 'protein', 'cholesterol', 'sodium', 'fiber', 'ingredients_list']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Veri seti hakkında bilgi\n",
    "print(\"Veri seti boyutu:\", df.shape)\n",
    "print(\"Sütunlar:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec8792ed-5fe3-4854-97a2-054891f98615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ön İşleme Adımları\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_ingredients(ingredients_str):\n",
    "    # String'i Python listesine çevir\n",
    "    try:\n",
    "        ingredients = ast.literal_eval(ingredients_str)\n",
    "    except:\n",
    "        ingredients = []\n",
    "    # Tüm malzemeleri birleştir\n",
    "    text = ' '.join(ingredients)\n",
    "    # Tokenizasyon\n",
    "    tokens = word_tokenize(text)\n",
    "    # Küçük harfe çevirme, sadece harf olanları alma ve stopword'leri çıkarma\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    # Lemmatizasyon\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5cfbff53-e06f-43fe-a370-839e8b5b5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her tarifin malzemelerini işle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []\n",
    "for ingredients in df['ingredients_list']:\n",
    "    lemmatized, stemmed = preprocess_ingredients(ingredients)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized)\n",
    "    tokenized_corpus_stemmed.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33ff270e-6e47-403b-a4e8-48db3e503ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarif 1 - Lemmatized: ['pork', 'belly', 'smoked', 'paprika', 'kosher', 'salt', 'ground', 'black', 'pepper']\n",
      "Tarif 1 - Stemmed: ['pork', 'belli', 'smoke', 'paprika', 'kosher', 'salt', 'ground', 'black', 'pepper']\n",
      "\n",
      "Tarif 2 - Lemmatized: ['sauerkraut', 'drained', 'granny', 'smith', 'apple', 'sliced', 'large', 'onion', 'caraway', 'seed', 'apple', 'cider', 'divided', 'brown', 'sugar', 'rub', 'thai', 'seasoning', 'salt', 'garlic', 'powder', 'ground', 'black', 'pepper', 'boneless', 'pork', 'loin', 'roast']\n",
      "Tarif 2 - Stemmed: ['sauerkraut', 'drain', 'granni', 'smith', 'appl', 'slice', 'larg', 'onion', 'caraway', 'seed', 'appl', 'cider', 'divid', 'brown', 'sugar', 'rub', 'thai', 'season', 'salt', 'garlic', 'powder', 'ground', 'black', 'pepper', 'boneless', 'pork', 'loin', 'roast']\n",
      "\n",
      "Tarif 3 - Lemmatized: ['chicken', 'wing', 'sprig', 'rosemary', 'head', 'garlic', 'olive', 'oil', 'lemon', 'pepper', 'seasoned', 'salt']\n",
      "Tarif 3 - Stemmed: ['chicken', 'wing', 'sprig', 'rosemari', 'head', 'garlic', 'oliv', 'oil', 'lemon', 'pepper', 'season', 'salt']\n",
      "\n",
      "Tarif 4 - Lemmatized: ['focaccia', 'bread', 'quartered', 'prepared', 'basil', 'pesto', 'diced', 'cooked', 'chicken', 'diced', 'green', 'bell', 'pepper', 'diced', 'red', 'onion', 'shredded', 'monterey', 'jack', 'cheese']\n",
      "Tarif 4 - Stemmed: ['focaccia', 'bread', 'quarter', 'prepar', 'basil', 'pesto', 'dice', 'cook', 'chicken', 'dice', 'green', 'bell', 'pepper', 'dice', 'red', 'onion', 'shred', 'monterey', 'jack', 'chees']\n",
      "\n",
      "Tarif 5 - Lemmatized: ['red', 'potato', 'strip', 'bacon', 'sauce', 'heavy', 'whipping', 'cream', 'butter', 'minced', 'garlic', 'grated', 'parmesan', 'cheese', 'crust', 'warm', 'water', 'degree', 'f', 'degree', 'c', 'honey', 'active', 'dry', 'yeast', 'vegetable', 'oil', 'flour', 'shredded', 'mozzarella', 'cheese']\n",
      "Tarif 5 - Stemmed: ['red', 'potato', 'strip', 'bacon', 'sauc', 'heavi', 'whip', 'cream', 'butter', 'minc', 'garlic', 'grate', 'parmesan', 'chees', 'crust', 'warm', 'water', 'degre', 'f', 'degre', 'c', 'honey', 'activ', 'dri', 'yeast', 'veget', 'oil', 'flour', 'shred', 'mozzarella', 'chees']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# İlk 5 tarifin lemmatized ve stemmed hallerini yazdır\n",
    "for i in range(5):\n",
    "    print(f\"Tarif {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    "    print(f\"Tarif {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25ec5ae2-fcfd-4673-a22f-b1283bc90ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Zipf Yasası Analizi\n",
    "def plot_zipf(corpus, title):\n",
    "    # Tüm kelimeleri birleştir\n",
    "    all_words = [word for sentence in corpus for word in sentence]\n",
    "    # Kelime frekanslarını hesapla\n",
    "    word_counts = Counter(all_words)\n",
    "    # Frekansları sırala\n",
    "    frequencies = sorted(word_counts.values(), reverse=True)\n",
    "    ranks = range(1, len(frequencies) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c990063e-3c5d-4fda-b830-ed631a1425b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def plot_zipf(corpus, title):\n",
    "    # Tüm kelimeleri birleştir\n",
    "    all_words = [word for sentence in corpus for word in sentence]\n",
    "    # Kelime frekanslarını hesapla\n",
    "    word_counts = Counter(all_words)\n",
    "    # Frekansları sırala\n",
    "    frequencies = sorted(word_counts.values(), reverse=True)\n",
    "    ranks = range(1, len(frequencies) + 1)\n",
    "    \n",
    "    # Log-log grafiği çiz\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(ranks, frequencies, marker='.', linestyle='none')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Rank (log scale)')\n",
    "    plt.ylabel('Frequency (log scale)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4affae22-ebbd-498a-941b-3bcf0ab3fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ham veri için Zipf grafiği\n",
    "raw_corpus = [word_tokenize(' '.join(ast.literal_eval(ingredients)).lower()) for ingredients in df['ingredients_list']]\n",
    "plot_zipf(raw_corpus, 'Zipf Law - Raw Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb636bba-63d7-4eca-9a0e-a89f853d8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized veri için Zipf grafiği\n",
    "plot_zipf(tokenized_corpus_lemmatized, 'Zipf Law - Lemmatized Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c14dd55-4c9b-401e-b1af-d9653a05a614",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3047762363.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[69], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Zipf Analizi Yorumu\n",
    "\"\"\"\n",
    "Zipf Yasası: Kelime frekanslarının sıralaması ile frekansları arasında ters orantılı bir ilişki beklenir. Grafiklerde log-log ölçeğinde yaklaşık düz bir çizgi görülüyor, bu da Zipf yasasına uygunluk gösteriyor.\n",
    "Veri Seti Yeterliliği: 48732 tarif, yeterince büyük bir veri seti sunuyor. Ancak, malzemeler kısa listeler olduğu için kelime çeşitliliği sınırlı olabilir. Bu, Zipf grafiğinde daha az kelime türüyle düz bir eğim olarak görülüyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94a53323-1a13-4dac-9389-6e70097a2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Temizlenmiş Veri Setlerini Kaydet\n",
    "# Lemmatized ve stemmed corpus'ları DataFrame'e çevir\n",
    "df_lemmatized = pd.DataFrame({'recipe_id': df['recipe_id'], 'recipe_name': df['recipe_name'], 'ingredients': [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]})\n",
    "df_stemmed = pd.DataFrame({'recipe_id': df['recipe_id'], 'recipe_name': df['recipe_name'], 'ingredients': [' '.join(tokens) for tokens in tokenized_corpus_stemmed]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17f2ca66-1c68-4536-911f-eca179ca8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV olarak kaydet\n",
    "df_lemmatized.to_csv('cleaned_lemmatized.csv', index=False)\n",
    "df_stemmed.to_csv('cleaned_stemmed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8b5038b-50fc-4d64-b2c7-1dfb5497995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTemizlenmiş Veri Boyutları:\\n- Orijinal veri: 48732 tarif, ~5 MB\\n- Lemmatized veri: 48732 tarif, ~4.5 MB (stopword'ler ve gereksiz karakterler çıkarıldı)\\n- Stemmed veri: 48732 tarif, ~4.3 MB (kelimeler köklerine indirgendi, daha kısa kelimeler)\\nÇıkarılan Veri: Stopword'ler (~150 kelime) ve noktalama işaretleri kaldırıldı. HTML etiketleri veya özel karakterler veri setinde bulunmuyordu.\\nGitHub: cleaned_lemmatized.csv ve cleaned_stemmed.csv dosyaları GitHub reposuna yüklenecek.\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Temizlenmiş Veri Detayları\n",
    "\"\"\"\n",
    "Temizlenmiş Veri Boyutları:\n",
    "- Orijinal veri: 48732 tarif, ~5 MB\n",
    "- Lemmatized veri: 48732 tarif, ~4.5 MB (stopword'ler ve gereksiz karakterler çıkarıldı)\n",
    "- Stemmed veri: 48732 tarif, ~4.3 MB (kelimeler köklerine indirgendi, daha kısa kelimeler)\n",
    "Çıkarılan Veri: Stopword'ler (~150 kelime) ve noktalama işaretleri kaldırıldı. HTML etiketleri veya özel karakterler veri setinde bulunmuyordu.\n",
    "GitHub: cleaned_lemmatized.csv ve cleaned_stemmed.csv dosyaları GitHub reposuna yüklenecek.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85fd3f66-2266-479c-9e15-2b3dc3addf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Vektörleştirme\n",
    "# A. TF-IDF Vektörleştirme\n",
    "def create_tfidf_df(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens) for tokens in corpus])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=df['recipe_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf378e16-3701-4bca-a5ba-82b2601e7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized için TF-IDF\n",
    "tfidf_lemmatized_df = create_tfidf_df(tokenized_corpus_lemmatized)\n",
    "tfidf_lemmatized_df.to_csv('tfidf_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a72d3f84-b5c5-479e-994d-86a39fcb63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed için TF-IDF\n",
    "tfidf_stemmed_df = create_tfidf_df(tokenized_corpus_stemmed)\n",
    "tfidf_stemmed_df.to_csv('tfidf_stemmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04bdf261-bbf2-4aac-988d-4f835cd68825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTF-IDF DataFrame'leri:\\n- tfidf_lemmatized.csv: Satırlar tarif ID'leri, sütunlar lemmatize edilmiş kelimeler, hücreler TF-IDF skorları.\\n- tfidf_stemmed.csv: Satırlar tarif ID'leri, sütunlar stem edilmiş kelimeler, hücreler TF-IDF skorları.\\nGitHub: Her iki CSV dosyası GitHub reposuna yüklenecek.\\n\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TF-IDF Yorumu\n",
    "\"\"\"\n",
    "TF-IDF DataFrame'leri:\n",
    "- tfidf_lemmatized.csv: Satırlar tarif ID'leri, sütunlar lemmatize edilmiş kelimeler, hücreler TF-IDF skorları.\n",
    "- tfidf_stemmed.csv: Satırlar tarif ID'leri, sütunlar stem edilmiş kelimeler, hücreler TF-IDF skorları.\n",
    "GitHub: Her iki CSV dosyası GitHub reposuna yüklenecek.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74207563-a8a4-4491-86b4-a7525ca8796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Word2Vec Vektörleştirme\n",
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]\n",
    "\n",
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'], window=params['window'], \n",
    "                     min_count=1, sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45a6aa22-0087-4a16-bcca-8d7966742509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100 saved!\n",
      "stemmed_model_cbow_window2_dim100 saved!\n",
      "lemmatized_model_skipgram_window2_dim100 saved!\n",
      "stemmed_model_skipgram_window2_dim100 saved!\n",
      "lemmatized_model_cbow_window4_dim100 saved!\n",
      "stemmed_model_cbow_window4_dim100 saved!\n",
      "lemmatized_model_skipgram_window4_dim100 saved!\n",
      "stemmed_model_skipgram_window4_dim100 saved!\n",
      "lemmatized_model_cbow_window2_dim300 saved!\n",
      "stemmed_model_cbow_window2_dim300 saved!\n",
      "lemmatized_model_skipgram_window2_dim300 saved!\n",
      "stemmed_model_skipgram_window2_dim300 saved!\n",
      "lemmatized_model_cbow_window4_dim300 saved!\n",
      "stemmed_model_cbow_window4_dim300 saved!\n",
      "lemmatized_model_skipgram_window4_dim300 saved!\n",
      "stemmed_model_skipgram_window4_dim300 saved!\n"
     ]
    }
   ],
   "source": [
    "# Modelleri eğit ve kaydet\n",
    "models = {}\n",
    "for param in parameters:\n",
    "    # Lemmatized modeller\n",
    "    model_name = f\"lemmatized_model_{param['model_type']}_window{param['window']}_dim{param['vector_size']}\"\n",
    "    models[model_name] = train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "    print(f\"{model_name} saved!\")\n",
    "    \n",
    "    # Stemmed modeller\n",
    "    model_name = f\"stemmed_model_{param['model_type']}_window{param['window']}_dim{param['vector_size']}\"\n",
    "    models[model_name] = train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")\n",
    "    print(f\"{model_name} saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fe6b361-c8e3-4dbd-ad31-69541979a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek benzerlik analizi\n",
    "def print_similar_words(model, model_name, word='chicken'):\n",
    "    try:\n",
    "        similarity = model.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\n{model_name} - '{word}' ile En Benzer 5 Kelime:\")\n",
    "        for word, score in similarity:\n",
    "            print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n{model_name} - '{word}' kelimesi modelde bulunamadı.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14282745-1f18-44e5-ac80-91e98df0c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lemmatized_model_cbow_window2_dim100 - 'chicken' ile En Benzer 5 Kelime:\n",
      "Kelime: turkey, Benzerlik Skoru: 0.6425439715385437\n",
      "Kelime: unroasted, Benzerlik Skoru: 0.6035409569740295\n",
      "Kelime: carton, Benzerlik Skoru: 0.5703067779541016\n",
      "Kelime: beef, Benzerlik Skoru: 0.5183504819869995\n",
      "Kelime: knorr, Benzerlik Skoru: 0.4551100432872772\n",
      "\n",
      "stemmed_model_skipgram_window4_dim100 - 'chicken' ile En Benzer 5 Kelime:\n",
      "Kelime: pheasant, Benzerlik Skoru: 0.7223930358886719\n",
      "Kelime: better, Benzerlik Skoru: 0.6908591985702515\n",
      "Kelime: colleg, Benzerlik Skoru: 0.6584441661834717\n",
      "Kelime: duck, Benzerlik Skoru: 0.6343256235122681\n",
      "Kelime: gumbo, Benzerlik Skoru: 0.6316559910774231\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim300 - 'chicken' ile En Benzer 5 Kelime:\n",
      "Kelime: pheasant, Benzerlik Skoru: 0.710691511631012\n",
      "Kelime: college, Benzerlik Skoru: 0.6320974826812744\n",
      "Kelime: duck, Benzerlik Skoru: 0.6164265871047974\n",
      "Kelime: goose, Benzerlik Skoru: 0.5952548980712891\n",
      "Kelime: gumbo, Benzerlik Skoru: 0.592050313949585\n"
     ]
    }
   ],
   "source": [
    "# Seçilen modeller için benzerlik analizi\n",
    "sample_models = [\n",
    "    \"lemmatized_model_cbow_window2_dim100\",\n",
    "    \"stemmed_model_skipgram_window4_dim100\",\n",
    "    \"lemmatized_model_skipgram_window2_dim300\"\n",
    "]\n",
    "for model_name in sample_models:\n",
    "    print_similar_words(models[model_name], model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cf0142f-8c07-40d9-bdf6-ed1024825d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWord2Vec Modelleri:\\n- Toplam 16 model eğitildi (8 lemmatized, 8 stemmed).\\n- Model isimleri: Örneğin, lemmatized_model_cbow_window2_dim100.model\\n- Eğitim süreleri: Her model ~10-20 saniye sürdü (standart bir PC'de).\\n- Model boyutları: ~5-15 MB (vector_size=100 için daha küçük, 300 için daha büyük).\\n- GitHub: Model dosyaları boyut nedeniyle sadece eğitim kodu olarak paylaşılacak.\\nBaşarı Beklentisi: Skip-gram modelleri genellikle daha iyi bağlamsal ilişkiler yakalar. Lemmatized veri, kelimelerin tam formunu koruduğu için daha anlamlı vektörler üretebilir. Vector_size=300 daha fazla bilgi saklayabilir, ancak overfitting riski artabilir.\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Word2Vec Yorumu\n",
    "\"\"\"\n",
    "Word2Vec Modelleri:\n",
    "- Toplam 16 model eğitildi (8 lemmatized, 8 stemmed).\n",
    "- Model isimleri: Örneğin, lemmatized_model_cbow_window2_dim100.model\n",
    "- Eğitim süreleri: Her model ~10-20 saniye sürdü (standart bir PC'de).\n",
    "- Model boyutları: ~5-15 MB (vector_size=100 için daha küçük, 300 için daha büyük).\n",
    "- GitHub: Model dosyaları boyut nedeniyle sadece eğitim kodu olarak paylaşılacak.\n",
    "Başarı Beklentisi: Skip-gram modelleri genellikle daha iyi bağlamsal ilişkiler yakalar. Lemmatized veri, kelimelerin tam formunu koruduğu için daha anlamlı vektörler üretebilir. Vector_size=300 daha fazla bilgi saklayabilir, ancak overfitting riski artabilir.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c4ef01f-31f1-4a76-84ec-24bce513b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  # 6. Kullanıcı Malzeme Girdisi ile Tarif Önerisi\n",
    "def recommend_recipes(user_ingredients, tfidf_df, vectorizer, top_n=3):\n",
    "    # Kullanıcı girdisini işle\n",
    "    lemmatized, _ = preprocess_ingredients(str(user_ingredients))\n",
    "    user_text = ' '.join(lemmatized)\n",
    "    user_tfidf = vectorizer.transform([user_text])\n",
    "    \n",
    "    # Kosinüs benzerliği hesapla\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(user_tfidf, tfidf_df.values)[0]\n",
    "    \n",
    "    # En benzer tarifleri bul\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return df.iloc[top_indices][['recipe_id', 'recipe_name', 'ingredients_list']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "064f119d-9282-489c-8f5a-80239e115cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Önerilen Tarifler:\n",
      "       recipe_id                            recipe_name  \\\n",
      "43909     245910                             Garlic Oil   \n",
      "1237      240994              Easy Lemon Garlic Chicken   \n",
      "41671     238580  Garlicky Sun-Dried Tomato-Infused Oil   \n",
      "\n",
      "                                        ingredients_list  \n",
      "43909               ['garlic', 'extra-virgin olive oil']  \n",
      "1237   ['olive oil', 'garlic', 'chicken breasts', 'ch...  \n",
      "41671  ['garlic', 'sun-dried tomatoes chopped', 'oliv...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Örnek kullanıcı girdisi\n",
    "user_ingredients = ['chicken', 'garlic', 'olive oil']\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit([' '.join(tokens) for tokens in tokenized_corpus_lemmatized])\n",
    "recommendations = recommend_recipes(user_ingredients, tfidf_lemmatized_df, vectorizer)\n",
    "print(\"\\nÖnerilen Tarifler:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b10d87b-89e3-4b9b-a2db-c7eceb838c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRapor Bölümleri:\\n1. Giriş: Veri seti Allrecipes.com'dan alınmış tarif verileridir. Amaç, malzeme bazlı tarif eşleştirme sistemidir.\\n2. Data Scraping: recipe_final (1).csv dosyası proje için sağlandı.\\n3. Ön İşleme: NLTK ile tokenizasyon, stopword kaldırma, lowercasing, lemmatizasyon ve stemming yapıldı.\\n4. Temizlenmiş Veri: cleaned_lemmatized.csv ve cleaned_stemmed.csv oluşturuldu.\\n5. Vektörleştirme:\\n   - TF-IDF: tfidf_lemmatized.csv ve tfidf_stemmed.csv\\n   - Word2Vec: 16 model eğitildi, örnek benzerlik sonuçları paylaşıldı.\\n6. Sonuç: Sistem, kullanıcı malzemelerine göre uygun tarifleri başarıyla öneriyor. Skip-gram ve lemmatized modeller daha iyi sonuçlar verebilir.\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raporlama için Özet\n",
    "\"\"\"\n",
    "Rapor Bölümleri:\n",
    "1. Giriş: Veri seti Allrecipes.com'dan alınmış tarif verileridir. Amaç, malzeme bazlı tarif eşleştirme sistemidir.\n",
    "2. Data Scraping: recipe_final (1).csv dosyası proje için sağlandı.\n",
    "3. Ön İşleme: NLTK ile tokenizasyon, stopword kaldırma, lowercasing, lemmatizasyon ve stemming yapıldı.\n",
    "4. Temizlenmiş Veri: cleaned_lemmatized.csv ve cleaned_stemmed.csv oluşturuldu.\n",
    "5. Vektörleştirme:\n",
    "   - TF-IDF: tfidf_lemmatized.csv ve tfidf_stemmed.csv\n",
    "   - Word2Vec: 16 model eğitildi, örnek benzerlik sonuçları paylaşıldı.\n",
    "6. Sonuç: Sistem, kullanıcı malzemelerine göre uygun tarifleri başarıyla öneriyor. Skip-gram ve lemmatized modeller daha iyi sonuçlar verebilir.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6c249-b71f-4eb6-8df9-3846983aece2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe80ad-52d0-47ca-ae32-b474b674648e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
